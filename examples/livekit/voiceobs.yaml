# voiceobs configuration file
# Documentation: https://github.com/voice-observation/voiceobs

# Exporter settings
exporters:
  # JSONL file exporter (for offline analysis)
  jsonl:
    enabled: true
    path: "./voiceobs_traces.jsonl"

  # Console exporter (prints spans to stdout)
  console:
    enabled: true

# Failure detection thresholds
failures:
  # Any overlap triggers an interruption failure
  interruption_overlap_ms: 0.0

  # Silence after user turn that triggers failure
  excessive_silence_ms: 3000.0

  # Stage latency thresholds
  slow_asr_ms: 2000.0
  slow_llm_ms: 2000.0
  slow_tts_ms: 2000.0

  # Quality thresholds (0.0 to 1.0)
  asr_min_confidence: 0.7
  llm_min_relevance: 0.5

  # Severity classification thresholds
  severity:
    interruption:
      low_max_ms: 200.0      # 0-200ms = LOW
      medium_max_ms: 500.0   # 200-500ms = MEDIUM, >500ms = HIGH

    silence:
      low_max_ms: 5000.0     # 3000-5000ms = LOW
      medium_max_ms: 8000.0  # 5000-8000ms = MEDIUM, >8000ms = HIGH

    slow_response:
      low_max_ms: 3000.0     # 2000-3000ms = LOW
      medium_max_ms: 5000.0  # 3000-5000ms = MEDIUM, >5000ms = HIGH

# Regression detection thresholds (for voiceobs compare)
regression:
  latency:
    warning_pct: 10.0   # Warn if latency increases by 10%
    critical_pct: 25.0  # Critical if latency increases by 25%

  silence:
    warning_pct: 15.0
    critical_pct: 30.0

  interruption_rate:
    warning_pct: 5.0
    critical_pct: 15.0

  intent_correct:
    warning_pct: 5.0    # Warn if correctness drops by 5%
    critical_pct: 15.0

  relevance:
    warning_pct: 10.0
    critical_pct: 20.0

# LLM evaluator settings (for semantic evaluation)
eval:
  # Provider: gemini, openai, or anthropic
  provider: "gemini"

  # Model name (null = use provider default)
  # Defaults: gemini-2.0-flash, gpt-4o-mini, claude-3-5-haiku-latest
  model: null

  # Sampling temperature (0.0 = deterministic)
  temperature: 0.0

  # Evaluation result caching
  cache:
    enabled: true
    dir: ".voiceobs_cache"
